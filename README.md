# MCP Agent Optimizer

å¤§è§„æ¨¡MCPæœåŠ¡é›†æˆçš„ä¼˜åŒ–æ¡†æ¶,å®ç°äº†æœ€æ–°çš„ç®—æ³•å’Œå·¥ç¨‹å®è·µ,ç”¨äºæ„å»ºé«˜æ•ˆã€æ™ºèƒ½çš„AI Agentç³»ç»Ÿã€‚

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

### ç®—æ³•ä¼˜åŒ–

1. **åˆ†å±‚è¯­ä¹‰è·¯ç”± (Hierarchical Semantic Routing)**
   - åŸºäºTool-to-Agent Retrievalè®ºæ–‡(2025)
   - ç»Ÿä¸€å‘é‡ç©ºé—´ä¸­çš„å·¥å…·å’ŒæœåŠ¡æ£€ç´¢
   - BM25 + Dense Vectoræ··åˆæ£€ç´¢
   - åœ¨å¤§è§„æ¨¡å·¥å…·åœºæ™¯ä¸‹æå‡19.4%çš„å‡†ç¡®ç‡

2. **å¼ºåŒ–å­¦ä¹ å·¥å…·é€‰æ‹© (RL-based Tool Selection)**
   - **è¿›æ­¥å¥–åŠ± (Progress Reward)**: å¯¹æ¯”ç›¸é‚»æ­¥éª¤çš„æ”¹è¿›,è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜
   - **è¿›ç¨‹å¥–åŠ±æ¨¡å‹ (Process Reward Model)**: ä¸ºæ¯ä¸€æ­¥æä¾›ç»†ç²’åº¦åé¦ˆ
   - **GRPOç®—æ³•**: Group Relative Policy Optimization,ç¨³å®šé«˜æ•ˆçš„ç­–ç•¥ä¼˜åŒ–
   - æ”¯æŒä»ä¸“å®¶æ¼”ç¤ºå­¦ä¹ (InversePRM)

3. **å¹¶è¡Œæ‰§è¡Œè§„åˆ’ (Parallel Execution Planning)**
   - åŸºäºLLMCompilerè®ºæ–‡(ICML 2024)
   - è‡ªåŠ¨æ„å»ºå·¥å…·è°ƒç”¨ä¾èµ–å›¾(DAG)
   - è¯†åˆ«å¹¶å¹¶è¡Œæ‰§è¡Œæ— ä¾èµ–ä»»åŠ¡
   - æœ€é«˜å¯å®ç°3.7å€åŠ é€Ÿ

### å·¥ç¨‹ä¼˜åŒ–

1. **æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ**
   - æç¤ºç¼“å­˜:å‡å°‘é‡å¤å¤„ç†é™æ€ä¸Šä¸‹æ–‡
   - å·¥å…·è°ƒç”¨ç¼“å­˜:é¿å…é‡å¤æ‰§è¡Œç›¸åŒå‚æ•°çš„è°ƒç”¨
   - LLMé©±åŠ¨çš„ç¼“å­˜å†³ç­–:è®©æ¨¡å‹è‡ªä¸»åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç¼“å­˜

2. **ä¸Šä¸‹æ–‡å·¥ç¨‹**
   - å³æ—¶(Just-in-Time)ä¸Šä¸‹æ–‡åŠ è½½
   - åŠ¨æ€å·¥å…·æè¿°ç®¡ç†
   - é¿å…ä¸Šä¸‹æ–‡è…åŒ–

3. **åˆ†å¸ƒå¼æ¶æ„æ”¯æŒ**
   - ç¼–æ’Agent + å·¥ä½œAgentæ¨¡å¼
   - é«˜å†…èšã€ä½è€¦åˆ
   - æ˜“äºæ‰©å±•å’Œç»´æŠ¤

## ğŸ“¦ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/shawnli/mcp-agent-optimizer.git
cd mcp-agent-optimizer

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…åŒ…
pip install -e .
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åˆ†å±‚è¯­ä¹‰è·¯ç”±

```python
from mcp_optimizer import HierarchicalRouter, MCPService, Tool

# åˆ›å»ºMCPæœåŠ¡å’Œå·¥å…·
email_service = MCPService(
    id="email_service",
    name="Email Service",
    description="Send and manage emails",
    category="communication"
)

search_tool = Tool(
    id="search_emails",
    name="Search Emails",
    description="Search emails by keywords",
    parent_service_id="email_service"
)

email_service.add_tool(search_tool)

# åˆ›å»ºè·¯ç”±å™¨
router = HierarchicalRouter(
    services=[email_service],
    bm25_weight=0.3,
    dense_weight=0.7
)

# è·¯ç”±æŸ¥è¯¢åˆ°æœ€ç›¸å…³çš„æœåŠ¡
query = "find emails about project updates"
top_services = router.route(query, top_k=3)

print(f"Top services: {[s.name for s in top_services]}")
```

### 2. è¿›æ­¥å¥–åŠ±è®¡ç®—

```python
from mcp_optimizer import ProgressReward

# åˆ›å»ºè¿›æ­¥å¥–åŠ±è®¡ç®—å™¨
progress_reward = ProgressReward()

# å®šä¹‰çŠ¶æ€
prev_state = {
    "completed_subtasks": 2,
    "total_subtasks": 5
}

curr_state = {
    "completed_subtasks": 3,
    "total_subtasks": 5
}

goal = "Complete all subtasks"

# è®¡ç®—è¿›æ­¥å¥–åŠ±
reward = progress_reward.compute_progress_reward(prev_state, curr_state, goal)
print(f"Progress reward: {reward}")  # æ­£å€¼è¡¨ç¤ºæœ‰è¿›æ­¥
```

### 3. å¹¶è¡Œæ‰§è¡Œè§„åˆ’

```python
from mcp_optimizer import ExecutionPlanner, ParallelExecutor

# åˆ›å»ºæ‰§è¡Œè®¡åˆ’
planner = ExecutionPlanner()

tool_calls = [
    {
        "task_id": "t1",
        "tool_id": "search_api",
        "parameters": {"query": "AI news"},
        "depends_on": []
    },
    {
        "task_id": "t2",
        "tool_id": "search_api",
        "parameters": {"query": "ML papers"},
        "depends_on": []
    },
    {
        "task_id": "t3",
        "tool_id": "summarize",
        "parameters": {"text": "$t1.result"},
        "depends_on": ["t1"]
    }
]

plan = planner.create_plan(tool_calls, available_tools)

# å¯è§†åŒ–DAG
print(planner.visualize_dag(plan))

# å¹¶è¡Œæ‰§è¡Œ
executor = ParallelExecutor(max_workers=10)
results = executor.execute_plan(plan, tools, planner)

print(f"Estimated speedup: {planner.estimate_speedup(plan):.2f}x")
```

### 4. æ™ºèƒ½ç¼“å­˜

```python
from mcp_optimizer import IntelligentCache, ToolCallCache

# åˆ›å»ºç¼“å­˜
tool_cache = ToolCallCache(max_size=5000, default_ttl=3600)
intelligent_cache = IntelligentCache(tool_cache)

# æ‰§è¡Œå·¥å…·è°ƒç”¨(è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜)
result, from_cache = intelligent_cache.execute_with_cache(
    tool_id="search_api",
    parameters={"query": "AI news"},
    execute_fn=lambda **kwargs: search_api.execute(**kwargs),
    context="User wants latest AI news"
)

print(f"Result from cache: {from_cache}")
print(f"Cache stats: {tool_cache.get_stats()}")
```

## ğŸ“š å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹`examples/`ç›®å½•è·å–æ›´å¤šç¤ºä¾‹:

- `basic_routing.py`: åˆ†å±‚è·¯ç”±çš„å®Œæ•´ç¤ºä¾‹
- `rl_tool_selection.py`: å¼ºåŒ–å­¦ä¹ å·¥å…·é€‰æ‹©
- `parallel_execution.py`: å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–

## ğŸ§ª è¿è¡Œç¤ºä¾‹

```bash
# åˆ†å±‚è·¯ç”±ç¤ºä¾‹
python examples/basic_routing.py

# RLå·¥å…·é€‰æ‹©ç¤ºä¾‹
python examples/rl_tool_selection.py

# å¹¶è¡Œæ‰§è¡Œç¤ºä¾‹
python examples/parallel_execution.py
```

## ğŸ“– æ ¸å¿ƒæ¦‚å¿µ

### è¿›æ­¥å¥–åŠ± (Progress Reward)

è¿™æ˜¯ç”¨æˆ·æå‡ºçš„åˆ›æ–°æƒ³æ³•,å¯¹åº”å¼ºåŒ–å­¦ä¹ ä¸­çš„æ ¸å¿ƒæ¦‚å¿µ:

**ç†è®ºåŸºç¡€**:
- **ä¼˜åŠ¿å‡½æ•°**: A(s,a) = Q(s,a) - V(s)
- **æ—¶åºå·®åˆ†**: TD_error = r_t + Î³*V(s_{t+1}) - V(s_t)
- **è¿›ç¨‹å¥–åŠ±**: è¯„ä¼°æ¯ä¸€æ­¥çš„è´¨é‡,è€Œéåªçœ‹æœ€ç»ˆç»“æœ

**å®è·µä»·å€¼**:
- âœ… è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜
- âœ… ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…
- âœ… æ¯ä¸€æ­¥éƒ½æœ‰å³æ—¶åé¦ˆ
- âœ… æ›´å¿«çš„å­¦ä¹ é€Ÿåº¦

**å®ç°æ–¹å¼**:
```python
# æ–¹æ³•1: çŠ¶æ€ä»·å€¼å¯¹æ¯”
V_prev = value_model(state_t-1)
V_curr = value_model(state_t)
progress_reward = V_curr - V_prev

# æ–¹æ³•2: ç›®æ ‡è·ç¦»å¯¹æ¯”
distance_prev = distance_to_goal(state_t-1, goal)
distance_curr = distance_to_goal(state_t, goal)
progress_reward = distance_prev - distance_curr

# æ–¹æ³•3: LLMè¯„åˆ¤
progress_score = llm_judge(state_t-1, state_t, goal)
progress_reward = normalize(progress_score)
```

### åˆ†å±‚è¯­ä¹‰è·¯ç”±

å°†æ‰å¹³çš„å·¥å…·åˆ—è¡¨é‡æ„ä¸ºåˆ†å±‚çš„çŸ¥è¯†å›¾è°±:

```
æŸ¥è¯¢ â†’ æ£€ç´¢Top-Nå®ä½“(å·¥å…·+æœåŠ¡) â†’ èšåˆåˆ°çˆ¶æœåŠ¡ â†’ é€‰æ‹©Top-KæœåŠ¡
```

**ä¼˜åŠ¿**:
- æœç´¢ç©ºé—´ä»O(N)é™åˆ°O(log N)
- é¿å…ä¸Šä¸‹æ–‡è¿‡è½½
- æ›´é«˜çš„é€‰æ‹©å‡†ç¡®æ€§

### å¹¶è¡Œæ‰§è¡Œè§„åˆ’

"å…ˆè§„åˆ’,åæ‰§è¡Œ"çš„èŒƒå¼:

```
LLMç”Ÿæˆè®¡åˆ’ â†’ æ„å»ºDAG â†’ æ‹“æ‰‘æ’åº â†’ æ‰¹æ¬¡å¹¶è¡Œæ‰§è¡Œ
```

**ä¼˜åŠ¿**:
- å¤§å¹…é™ä½å»¶è¿Ÿ(æœ€é«˜3.7å€)
- å‡å°‘LLMè°ƒç”¨æ¬¡æ•°(æœ€é«˜6.7å€)
- å…¨å±€è§†é‡,è¯†åˆ«å¹¶è¡Œæœºä¼š

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### æ”¯æŒçš„ç®—æ³•

- **è·¯ç”±**: BM25, Dense Vector, Hybrid Retrieval
- **RL**: PPO, GRPO, Monte Carlo, GAE
- **å¥–åŠ±**: Outcome, Process, Progress, Contrastive, Shaped
- **ç¼“å­˜**: LRU, LFU, TTL, LLM-driven

### æ€§èƒ½æŒ‡æ ‡

åŸºäºç›¸å…³è®ºæ–‡çš„å®éªŒç»“æœ:

| æŒ‡æ ‡ | ä¼ ç»Ÿæ–¹æ³• | ä¼˜åŒ–å | æå‡ |
|:---|:---|:---|:---|
| å·¥å…·é€‰æ‹©å‡†ç¡®ç‡ | åŸºçº¿ | +19.4% | Tool-to-Agent Retrieval |
| æ‰§è¡Œå»¶è¿Ÿ | åŸºçº¿ | -73% (3.7x) | LLMCompiler |
| LLMè°ƒç”¨æˆæœ¬ | åŸºçº¿ | -85% (6.7x) | LLMCompiler |
| æ ·æœ¬æ•ˆç‡ | åŸºçº¿ | æ˜¾è‘—æå‡ | AgentPRM |

## ğŸ“„ å‚è€ƒæ–‡çŒ®

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å‰æ²¿ç ”ç©¶:

1. **LLMCompiler** (ICML 2024): An LLM Compiler for Parallel Function Calling
2. **Tool-to-Agent Retrieval** (2025): Bridging Tools and Agents for Scalable LLM Multi-Agent Systems
3. **AgentPRM** (2025): Process Reward Models for LLM Agents
4. **LLM-dCache** (HiPC 2024): Improving tool-augmented LLMs with GPT-driven localized data caching
5. **VisTA** (2025): A Reinforcement Learning Framework for Visual Tool Selection

å®Œæ•´å‚è€ƒæ–‡çŒ®è§[ç ”ç©¶æŠ¥å‘Š](docs/mcp_optimization_report.md)ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®!è¯·æŸ¥çœ‹[CONTRIBUTING.md](CONTRIBUTING.md)äº†è§£è¯¦æƒ…ã€‚

## ğŸ“ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ç›¸å…³è®ºæ–‡çš„ä½œè€…,ä»¥åŠæå‡º"è¿›æ­¥å¥–åŠ±"åˆ›æ–°æƒ³æ³•çš„ç”¨æˆ·ã€‚

## ğŸ“§ è”ç³»

- Issues: https://github.com/shawnli/mcp-agent-optimizer/issues
- Email: shawnli@example.com

---

**æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ªç ”ç©¶åŸå‹,éƒ¨åˆ†æ¨¡å—(å¦‚ç¥ç»ç½‘ç»œè®­ç»ƒ)éœ€è¦æ ¹æ®å®é™…éœ€æ±‚è¿›ä¸€æ­¥å®Œå–„ã€‚
